{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4597af82-1b24-4ad0-8efe-2c90be4ba6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka Brokers: kafk-1:9092,kafk-2:9092,kafk-3:9092\n",
      "Kafka Topic: raw_traffic_data\n",
      "Redis Host: redis:6379\n",
      "ClickHouse Host: clickhouse-server:9000\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import thư viện và lấy biến môi trường\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window, avg, count, udf, lit, current_timestamp, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, FloatType, IntegerType\n",
    "\n",
    "# --- Lấy cấu hình từ Biến Môi trường ---\n",
    "# Lưu ý: Các biến này được set trong docker-compose.yml cho service spark-jupyter\n",
    "KAFKA_BROKERS = os.environ.get(\"KAFKA_BROKERS\", \"kafk-1:9092,kafk-2:9092,kafk-3:9092\")\n",
    "KAFKA_TOPIC = os.environ.get(\"KAFKA_TOPIC\", \"raw_traffic_data\")\n",
    "REDIS_HOST = os.environ.get(\"REDIS_HOST\", \"redis\")\n",
    "REDIS_PORT = int(os.environ.get(\"REDIS_PORT\", 6379))\n",
    "CLICKHOUSE_HOST = os.environ.get(\"CLICKHOUSE_HOST\", \"clickhouse-server\")\n",
    "CLICKHOUSE_NATIVE_PORT = int(os.environ.get(\"CLICKHOUSE_NATIVE_PORT\", 9000))\n",
    "CLICKHOUSE_DB = \"traffic_db\"\n",
    "CLICKHOUSE_TABLE = \"traffic_events\"\n",
    "\n",
    "print(f\"Kafka Brokers: {KAFKA_BROKERS}\")\n",
    "print(f\"Kafka Topic: {KAFKA_TOPIC}\")\n",
    "print(f\"Redis Host: {REDIS_HOST}:{REDIS_PORT}\")\n",
    "print(f\"ClickHouse Host: {CLICKHOUSE_HOST}:{CLICKHOUSE_NATIVE_PORT}\")\n",
    "\n",
    "# --- Schema cho dữ liệu JSON từ Kafka ---\n",
    "schema = StructType([\n",
    "    StructField(\"vehicle_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"speed\", DoubleType(), True),\n",
    "    StructField(\"segment_id_actual\", StringType(), True) # Giữ lại để tham khảo/debug\n",
    "])\n",
    "\n",
    "# --- Dữ liệu đoạn đường tĩnh (Ví dụ - Nên tải từ file hoặc nguồn khác) ---\n",
    "SEGMENTS_COORDS = {\n",
    "    \"segment_1\": (10.98, 106.75, \"Ngã tư 550 Area\"), # lat, lon, name\n",
    "    \"segment_2\": (10.94, 106.81, \"KCN Sóng Thần Area\"),\n",
    "    \"segment_3\": (10.96, 106.78, \"Trung tâm Dĩ An Area\")\n",
    "}\n",
    "# Có thể tạo Spark DataFrame từ đây để join nếu cần làm giàu dữ liệu\n",
    "# segments_sdf = spark.createDataFrame([(k, v[0], v[1], v[2]) for k, v in SEGMENTS_COORDS.items()], [\"segment_id\", \"lat\", \"lon\", \"name\"])\n",
    "# segments_sdf.show()\n",
    "\n",
    "# --- Các hàm tiện ích (Có thể tách ra utils.py) ---\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # (Giữ nguyên hàm haversine từ trước)\n",
    "    R = 6371 # km\n",
    "    dLat = math.radians(lat2 - lat1)\n",
    "    dLon = math.radians(lon2 - lon1)\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    a = math.sin(dLat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dLon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def find_nearest_segment(lat, lon):\n",
    "    # (Giữ nguyên hàm map matching đơn giản)\n",
    "    min_dist = float('inf')\n",
    "    nearest_segment = \"unknown\"\n",
    "    if lat is None or lon is None:\n",
    "        return nearest_segment\n",
    "\n",
    "    for segment_id, (seg_lat, seg_lon, _) in SEGMENTS_COORDS.items(): # Lấy tọa độ từ dict\n",
    "        dist = haversine(lon, lat, seg_lon, seg_lat)\n",
    "        # Ngưỡng khoảng cách tối đa để map (ví dụ: 1km)\n",
    "        if dist < min_dist and dist < 1.0:\n",
    "            min_dist = dist\n",
    "            nearest_segment = segment_id\n",
    "    return nearest_segment\n",
    "\n",
    "find_nearest_segment_udf = udf(find_nearest_segment, StringType())\n",
    "\n",
    "def classify_status(avg_speed):\n",
    "    # (Giữ nguyên hàm phân loại)\n",
    "    if avg_speed is None:\n",
    "        return \"unknown\"\n",
    "    elif avg_speed > 45: # Điều chỉnh ngưỡng tốc độ\n",
    "        return \"clear\"\n",
    "    elif avg_speed > 20:\n",
    "        return \"slow\"\n",
    "    else:\n",
    "        return \"congested\"\n",
    "\n",
    "classify_status_udf = udf(classify_status, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adde7743-5af2-44d2-a845-94585059b591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuring Spark Session ---\n",
      "Attempting to include packages: org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.3,org.apache.hadoop:hadoop-aws:3.3.2\n",
      "Spark Session created successfully. Spark version: 3.5.0\n",
      "Actual spark.jars.packages config: org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.3,org.apache.hadoop:hadoop-aws:3.3.2\n",
      "--- Spark Session Ready ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Khởi tạo Spark Session (Cập nhật)\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# --- Xác định các gói Maven cần thiết ---\n",
    "# Phiên bản phải tương thích với phiên bản Spark trong image Jupyter\n",
    "# Ví dụ: image jupyter/pyspark-notebook:spark-3.3.3 dùng Spark 3.3.3 (Scala 2.12)\n",
    "SPARK_VERSION = \"3.3.3\" # Hoặc version khác tương ứng với base image bạn chọn\n",
    "SCALA_VERSION = \"2.12\"\n",
    "KAFKA_PACKAGE = f\"org.apache.spark:spark-sql-kafka-0-10_{SCALA_VERSION}:{SPARK_VERSION}\"\n",
    "# Gói hadoop-aws cần cho S3A (tương tác MinIO, ví dụ cho checkpoint)\n",
    "# Phiên bản Hadoop thường đi kèm Spark (kiểm tra nếu cần)\n",
    "HADOOP_VERSION = \"3.3.2\" # Thường tương thích với Spark 3.3.x\n",
    "AWS_PACKAGE = f\"org.apache.hadoop:hadoop-aws:{HADOOP_VERSION}\"\n",
    "# Đôi khi hadoop-aws cần thư viện phụ thuộc khác\n",
    "# AWS_SDK_PACKAGE = \"com.amazonaws:aws-java-sdk-bundle:1.11.1026\" # Ví dụ\n",
    "\n",
    "# Kết hợp các package lại\n",
    "spark_packages = f\"{KAFKA_PACKAGE},{AWS_PACKAGE}\"\n",
    "# Nếu cần thư viện phụ khác: spark_packages = f\"{KAFKA_PACKAGE},{AWS_PACKAGE},{AWS_SDK_PACKAGE}\"\n",
    "\n",
    "print(f\"--- Configuring Spark Session ---\")\n",
    "print(f\"Attempting to include packages: {spark_packages}\")\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"JupyterTrafficProcessor\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.jars.packages\", spark_packages) \\\n",
    "        .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/spark-checkpoints-jupyter\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", os.environ.get(\"MINIO_ENDPOINT\", \"http://minio:9000\")) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", os.environ.get(\"MINIO_ACCESS_KEY\", \"minioadmin\")) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ.get(\"MINIO_SECRET_KEY\", \"minioadmin\")) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Set log level\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(f\"Spark Session created successfully. Spark version: {spark.version}\")\n",
    "    # In ra cấu hình packages thực tế để kiểm tra\n",
    "    print(f\"Actual spark.jars.packages config: {spark.conf.get('spark.jars.packages')}\")\n",
    "    print(f\"--- Spark Session Ready ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!!! Error creating Spark Session: {e}\")\n",
    "    # Dừng thực thi nếu không tạo được session\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817ba92c-92f3-4c37-9baf-46ec8fc93039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading stream from Kafka topic 'raw_traffic_data' at brokers 'kafk-1:9092,kafk-2:9092,kafk-3:9092'\n",
      "Kafka stream schema:\n",
      "root\n",
      " |-- vehicle_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- speed: float (nullable = true)\n",
      " |-- segment_id_actual: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Đọc dữ liệu từ Kafka\n",
    "print(f\"Reading stream from Kafka topic '{KAFKA_TOPIC}' at brokers '{KAFKA_BROKERS}'\")\n",
    "\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKERS) \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON và chọn các cột cần thiết, chuyển đổi kiểu dữ liệu\n",
    "value_df = kafka_df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\n",
    "                   .select(\"data.*\") \\\n",
    "                   .withColumn(\"event_time\", col(\"timestamp\").cast(TimestampType())) \\\n",
    "                   .withColumn(\"latitude\", col(\"latitude\").cast(DoubleType())) \\\n",
    "                   .withColumn(\"longitude\", col(\"longitude\").cast(DoubleType())) \\\n",
    "                   .withColumn(\"speed\", col(\"speed\").cast(FloatType())) \\\n",
    "                   .filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull() & col(\"speed\").isNotNull()) # Lọc dữ liệu null cơ bản\n",
    "\n",
    "print(\"Kafka stream schema:\")\n",
    "value_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f27f43f1-07af-4a96-ab27-81a7ae75051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated stream schema:\n",
      "root\n",
      " |-- segment_id: string (nullable = true)\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      " |-- avg_speed: float (nullable = true)\n",
      " |-- vehicle_count: integer (nullable = false)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Xử lý dữ liệu (Map Matching, Aggregation, Classification)\n",
    "\n",
    "# 1. Map Matching\n",
    "mapped_df = value_df.withColumn(\"segment_id\", find_nearest_segment_udf(col(\"latitude\"), col(\"longitude\"))) \\\n",
    "                    .filter(col(\"segment_id\") != \"unknown\") # Chỉ giữ lại các điểm map được\n",
    "\n",
    "# (Tùy chọn) Thêm Data Quality Check (ví dụ lọc tốc độ bất thường)\n",
    "# mapped_df = mapped_df.filter((col(\"speed\") > 0) & (col(\"speed\") < 150))\n",
    "\n",
    "# (Tùy chọn) Enrichment - Join với dữ liệu tĩnh về segment (nếu có segments_sdf)\n",
    "# enriched_df = mapped_df.join(segments_sdf, \"segment_id\", \"left_outer\")\n",
    "\n",
    "# 2. Windowing & Aggregation\n",
    "# Sử dụng df đã map (hoặc enriched nếu có)\n",
    "agg_df = mapped_df \\\n",
    "    .withWatermark(\"event_time\", \"2 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 minute\", \"30 seconds\").alias(\"time_window\"), # Cửa sổ 1 phút, trượt 30s\n",
    "        col(\"segment_id\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        avg(\"speed\").alias(\"avg_speed\"),\n",
    "        count(\"*\").alias(\"vehicle_count\")\n",
    "    )\n",
    "\n",
    "# 3. Phân loại trạng thái\n",
    "status_df = agg_df.withColumn(\"status\", classify_status_udf(col(\"avg_speed\"))) \\\n",
    "                  .select(\n",
    "                      col(\"segment_id\"),\n",
    "                      col(\"time_window.start\").alias(\"window_start\"),\n",
    "                      col(\"time_window.end\").alias(\"window_end\"),\n",
    "                      col(\"avg_speed\").cast(FloatType()), # Đảm bảo đúng kiểu dữ liệu\n",
    "                      col(\"vehicle_count\").cast(IntegerType()), # Đảm bảo đúng kiểu dữ liệu\n",
    "                      col(\"status\")\n",
    "                   )\n",
    "\n",
    "\n",
    "print(\"Aggregated stream schema:\")\n",
    "status_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3a4416-21f6-441b-b579-d94e9252aae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming queries started. Check Spark UI (if accessible) and output logs.\n",
      "--- Redis Write (Epoch: 11) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 41.51, \"vehicle_count\": 23, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:19:30\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 35.62, \"vehicle_count\": 25, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:19:30\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 31.74, \"vehicle_count\": 26, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:19:30\"}\n",
      "--- ClickHouse Write (Epoch: 11) ---\n",
      "  Inserting 6 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 6 rows.\n",
      "--- Redis Write (Epoch: 12) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 31.38, \"vehicle_count\": 6, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:21:30\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 29.67, \"vehicle_count\": 5, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:21:30\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 45.99, \"vehicle_count\": 8, \"status\": \"clear\", \"window_end\": \"2025-04-20T20:21:30\"}\n",
      "--- ClickHouse Write (Epoch: 12) ---\n",
      "  Inserting 18 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 18 rows.\n",
      "--- Redis Write (Epoch: 13) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 41.0, \"vehicle_count\": 29, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:21:30\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 32.02, \"vehicle_count\": 27, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:21:30\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 44.52, \"vehicle_count\": 21, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:21:30\"}\n",
      "--- ClickHouse Write (Epoch: 13) ---\n",
      "  Inserting 6 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 6 rows.\n",
      "--- Redis Write (Epoch: 14) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 46.53, \"vehicle_count\": 18, \"status\": \"clear\", \"window_end\": \"2025-04-20T20:22:00\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 43.29, \"vehicle_count\": 14, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:22:00\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 33.91, \"vehicle_count\": 18, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:22:00\"}\n",
      "--- ClickHouse Write (Epoch: 14) ---\n",
      "  Inserting 9 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 9 rows.\n",
      "--- Redis Write (Epoch: 15) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 47.53, \"vehicle_count\": 8, \"status\": \"clear\", \"window_end\": \"2025-04-20T20:22:30\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 35.25, \"vehicle_count\": 9, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:22:30\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 38.37, \"vehicle_count\": 10, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:22:30\"}\n",
      "--- ClickHouse Write (Epoch: 15) ---\n",
      "  Inserting 9 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 9 rows.\n",
      "--- Redis Write (Epoch: 16) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 35.66, \"vehicle_count\": 28, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:22:30\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 39.03, \"vehicle_count\": 33, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:22:30\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 39.0, \"vehicle_count\": 34, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:22:30\"}\n",
      "--- ClickHouse Write (Epoch: 16) ---\n",
      "  Inserting 6 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 6 rows.\n",
      "--- Redis Write (Epoch: 17) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 39.8, \"vehicle_count\": 16, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:23:00\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 36.19, \"vehicle_count\": 24, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:23:00\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 35.88, \"vehicle_count\": 16, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:23:00\"}\n",
      "--- ClickHouse Write (Epoch: 17) ---\n",
      "  Inserting 8 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 8 rows.\n",
      "--- Redis Write (Epoch: 18) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 19.59, \"vehicle_count\": 5, \"status\": \"congested\", \"window_end\": \"2025-04-20T20:23:30\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 32.43, \"vehicle_count\": 9, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:23:30\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 39.92, \"vehicle_count\": 6, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:23:30\"}\n",
      "--- ClickHouse Write (Epoch: 18) ---\n",
      "  Inserting 9 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 9 rows.\n",
      "--- Redis Write (Epoch: 19) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 26.06, \"vehicle_count\": 18, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:23:30\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 38.73, \"vehicle_count\": 30, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:23:30\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 37.73, \"vehicle_count\": 23, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:23:30\"}\n",
      "--- ClickHouse Write (Epoch: 19) ---\n",
      "  Inserting 6 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 6 rows.\n",
      "--- Redis Write (Epoch: 20) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 32.36, \"vehicle_count\": 7, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:24:00\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 26.67, \"vehicle_count\": 6, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:24:00\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 35.72, \"vehicle_count\": 12, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:24:00\"}\n",
      "--- ClickHouse Write (Epoch: 20) ---\n",
      "  Inserting 9 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 9 rows.\n",
      "--- Redis Write (Epoch: 21) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 29.15, \"vehicle_count\": 25, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:24:00\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 34.36, \"vehicle_count\": 30, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:24:00\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 34.33, \"vehicle_count\": 29, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:24:00\"}\n",
      "--- ClickHouse Write (Epoch: 21) ---\n",
      "  Inserting 6 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 6 rows.\n",
      "--- Redis Write (Epoch: 22) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 43.76, \"vehicle_count\": 21, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:24:30\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 34.37, \"vehicle_count\": 12, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:24:30\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 32.2, \"vehicle_count\": 25, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:24:30\"}\n",
      "--- ClickHouse Write (Epoch: 22) ---\n",
      "  Inserting 9 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 9 rows.\n",
      "--- Redis Write (Epoch: 23) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 40.77, \"vehicle_count\": 15, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:25:00\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 33.7, \"vehicle_count\": 20, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:25:00\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 42.2, \"vehicle_count\": 11, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:25:00\"}\n",
      "--- ClickHouse Write (Epoch: 23) ---\n",
      "  Inserting 9 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 9 rows.\n",
      "--- Redis Write (Epoch: 24) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 46.48, \"vehicle_count\": 8, \"status\": \"clear\", \"window_end\": \"2025-04-20T20:25:30\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 43.26, \"vehicle_count\": 13, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:25:30\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 29.77, \"vehicle_count\": 10, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:25:30\"}\n",
      "--- ClickHouse Write (Epoch: 24) ---\n",
      "  Inserting 9 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 9 rows.\n",
      "--- Redis Write (Epoch: 25) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 53.34, \"vehicle_count\": 1, \"status\": \"clear\", \"window_end\": \"2025-04-20T20:26:00\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 37.09, \"vehicle_count\": 8, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:26:00\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 29.6, \"vehicle_count\": 5, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:26:00\"}\n",
      "--- ClickHouse Write (Epoch: 25) ---\n",
      "  Inserting 9 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 9 rows.\n",
      "--- Redis Write (Epoch: 26) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 40.92, \"vehicle_count\": 21, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:26:00\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 34.01, \"vehicle_count\": 38, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:26:00\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 33.45, \"vehicle_count\": 30, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:26:00\"}\n",
      "--- ClickHouse Write (Epoch: 26) ---\n",
      "  Inserting 6 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 6 rows.\n",
      "--- Redis Write (Epoch: 27) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 38.52, \"vehicle_count\": 29, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:26:30\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 39.06, \"vehicle_count\": 29, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:26:30\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 31.59, \"vehicle_count\": 18, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:26:30\"}\n",
      "--- ClickHouse Write (Epoch: 27) ---\n",
      "  Inserting 9 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 9 rows.\n",
      "--- Redis Write (Epoch: 28) ---\n",
      "  Updating Redis: segment:segment_3 -> {\"avg_speed\": 37.3, \"vehicle_count\": 18, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:27:00\"}\n",
      "  Updating Redis: segment:segment_2 -> {\"avg_speed\": 41.76, \"vehicle_count\": 28, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:27:00\"}\n",
      "  Updating Redis: segment:segment_1 -> {\"avg_speed\": 36.22, \"vehicle_count\": 27, \"status\": \"slow\", \"window_end\": \"2025-04-20T20:27:00\"}\n",
      "--- ClickHouse Write (Epoch: 28) ---\n",
      "  Inserting 9 rows into ClickHouse table 'traffic_events'...\n",
      "  Successfully inserted 9 rows.\n",
      "--- Redis Write (Epoch: 29) ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Ghi dữ liệu vào Redis và ClickHouse (Dùng foreachBatch)\n",
    "\n",
    "# --- Hàm ghi vào Redis ---\n",
    "def write_to_redis(df, epoch_id):\n",
    "    # Sử dụng df đã collect(), không nên collect trong production với dữ liệu lớn\n",
    "    # Nên dùng df.foreachPartition thay thế để xử lý song song\n",
    "    # Ví dụ này dùng collect() cho đơn giản trong notebook\n",
    "    if df.isEmpty():\n",
    "        return\n",
    "\n",
    "    import redis # import bên trong để tránh lỗi serialization\n",
    "\n",
    "    print(f\"--- Redis Write (Epoch: {epoch_id}) ---\")\n",
    "    r = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, db=0, decode_responses=True)\n",
    "    pipe = r.pipeline()\n",
    "    try:\n",
    "        # Chỉ lấy bản ghi mới nhất cho mỗi segment trong batch này\n",
    "        # Dùng RDD để xử lý linh hoạt hơn\n",
    "        latest_updates = df.rdd \\\n",
    "            .map(lambda row: (row.segment_id, row)) \\\n",
    "            .reduceByKey(lambda row1, row2: row1 if row1.window_end > row2.window_end else row2) \\\n",
    "            .map(lambda item: item[1]) \\\n",
    "            .collect()\n",
    "\n",
    "        for row in latest_updates:\n",
    "            segment_id = row.segment_id\n",
    "            state = {\n",
    "                \"avg_speed\": round(row.avg_speed, 2) if row.avg_speed else None,\n",
    "                \"vehicle_count\": row.vehicle_count,\n",
    "                \"status\": row.status,\n",
    "                \"window_end\": row.window_end.isoformat()\n",
    "            }\n",
    "            key = f\"segment:{segment_id}\"\n",
    "            value = json.dumps(state)\n",
    "            print(f\"  Updating Redis: {key} -> {value}\")\n",
    "            pipe.set(key, value)\n",
    "        pipe.execute()\n",
    "    except Exception as e:\n",
    "        print(f\"  Error writing to Redis: {e}\")\n",
    "\n",
    "# --- Hàm ghi vào ClickHouse ---\n",
    "def write_to_clickhouse(df, epoch_id):\n",
    "    if df.isEmpty():\n",
    "        return\n",
    "\n",
    "    # Sử dụng thư viện clickhouse-connect (hoặc clickhouse-driver)\n",
    "    import clickhouse_connect # import bên trong\n",
    "\n",
    "    print(f\"--- ClickHouse Write (Epoch: {epoch_id}) ---\")\n",
    "    try:\n",
    "        # Lấy dữ liệu dưới dạng list of tuples/dicts\n",
    "        # Cần đảm bảo tên cột và thứ tự khớp với bảng ClickHouse\n",
    "        data_to_insert = df.select(\n",
    "            \"segment_id\",\n",
    "            \"window_start\",\n",
    "            \"window_end\",\n",
    "            \"avg_speed\",\n",
    "            \"vehicle_count\",\n",
    "            \"status\"\n",
    "            # processing_time sẽ được ClickHouse tự thêm DEFAULT now()\n",
    "        ).collect() # Collect() không tốt cho production lớn!\n",
    "\n",
    "        if not data_to_insert:\n",
    "            print(\"  No data to insert.\")\n",
    "            return\n",
    "\n",
    "        # Chuyển đổi Row thành list/tuple nếu cần bởi client\n",
    "        # clickhouse-connect có thể nhận list of lists hoặc list of dicts\n",
    "        data_list = [list(row) for row in data_to_insert]\n",
    "        # Hoặc data_dict = [row.asDict() for row in data_to_insert]\n",
    "\n",
    "\n",
    "        # Kết nối và insert\n",
    "        client = clickhouse_connect.get_client(\n",
    "            host=CLICKHOUSE_HOST,\n",
    "            port=8123, # Dùng port Native TCP\n",
    "            database=CLICKHOUSE_DB\n",
    "            # Thêm username/password nếu ClickHouse có yêu cầu\n",
    "        )\n",
    "        print(f\"  Inserting {len(data_list)} rows into ClickHouse table '{CLICKHOUSE_TABLE}'...\")\n",
    "        # Tên cột cần khớp chính xác thứ tự trong data_list\n",
    "        client.insert(CLICKHOUSE_TABLE, data_list,\n",
    "                      column_names=['segment_id', 'window_start', 'window_end', 'avg_speed', 'vehicle_count', 'status'])\n",
    "        print(f\"  Successfully inserted {len(data_list)} rows.\")\n",
    "        client.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error writing to ClickHouse: {e}\")\n",
    "        # Có thể thêm logic retry hoặc ghi log chi tiết hơn\n",
    "\n",
    "\n",
    "# --- Khởi chạy Streaming Queries ---\n",
    "# Output Mode 'update' thường dùng cho foreachBatch khi không có aggregation hoàn chỉnh\n",
    "# Hoặc 'complete' nếu aggregation cho phép (ví dụ: count toàn bộ)\n",
    "# 'append' nếu chỉ ghi dữ liệu mới không cập nhật state cũ\n",
    "\n",
    "# Query ghi vào Redis và ClickHouse\n",
    "combined_write_query = status_df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark-checkpoints/combined_write\")\\\n",
    "    .foreachBatch(lambda df, epoch_id: [write_to_redis(df, epoch_id), write_to_clickhouse(df, epoch_id)]) \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "# (Tùy chọn) Query ghi ra console để debug\n",
    "# console_query = status_df.writeStream \\\n",
    "#     .outputMode(\"update\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .option(\"truncate\", \"false\") \\\n",
    "#     .option(\"numRows\", 50) \\\n",
    "#     .start()\n",
    "\n",
    "print(\"Streaming queries started. Check Spark UI (if accessible) and output logs.\")\n",
    "# Trong Jupyter, query sẽ chạy ngầm. Để dừng, bạn cần dùng:\n",
    "# combined_write_query.stop()\n",
    "# console_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da1141-30d1-4dcb-a4b8-da576e1ebc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
