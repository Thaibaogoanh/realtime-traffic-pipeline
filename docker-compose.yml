networks:
  traffic_net:
    driver: bridge

volumes:
  minio_data:
  clickhouse_data:
  postgres_db_data: # Volume cho Airflow DB
  airflow_dags:     # Volume cho DAGs
  airflow_logs:     # Volume cho Logs

services:
  # --- Cơ sở dữ liệu cho Airflow ---
  postgres:
    image: postgres:14-alpine # Chọn phiên bản Postgres
    container_name: postgres_airflow
    environment:
      - POSTGRES_USER=${AIRFLOW_DB_USER:-airflow}
      - POSTGRES_PASSWORD=${AIRFLOW_DB_PASSWORD:-airflow}
      - POSTGRES_DB=${AIRFLOW_DB_NAME:-airflow}
    volumes:
      - postgres_db_data:/var/lib/postgresql/data
    networks:
      - traffic_net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${AIRFLOW_DB_USER:-airflow} -d ${AIRFLOW_DB_NAME:-airflow}"]
      interval: 5s
      timeout: 5s
      retries: 5

  # --- Schema Registry ---
  schema-registry:
    image: confluentinc/cp-schema-registry:7.3.0 # Đồng bộ version với Kafka nếu có thể
    container_name: schema-registry
    depends_on:
      - kafk-1 # Chỉ cần phụ thuộc vào 1 node Kafka để lấy metadata ban đầu
      - kafk-2
      - kafk-3
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafk-1:9092,kafk-2:9092,kafk-3:9092 # Dùng listener nội bộ
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 3 # Đảm bảo отказоустойчивость cho schema topic
    networks:
      - traffic_net

  # --- Kafka Cluster (Giữ nguyên như trước) ---
  kafk-1:
    image: apache/kafka:3.7.0 # Sử dụng phiên bản bạn cung cấp hoặc mới hơn
    container_name: kafk-1
    hostname: kafk-1
    networks:
      - traffic_net
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_NODE_ID=1
      - KAFKA_PROCESS_ROLES=broker,controller
      - KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CONTROLLER_QUORUM_VOTERS=1@kafk-1:9093,2@kafk-2:9093,3@kafk-3:9093
      - KAFKA_LISTENERS=INTERNAL://:9092,CONTROLLER://:9093,PLAINTEXT://:9094
      - KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafk-1:9092,PLAINTEXT://localhost:29092 # Chỉ advertise internal và PLAINTEXT ra ngoài
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=3 # Tăng replication factor cho cluster
      - KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0
      - KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=2 # Tăng min ISR
      - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3 # Tăng replication factor
      - KAFKA_LOG_DIRS=/tmp/kraft-combined_logs
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
    ports:
      - "29092:9094" # Map cổng PLAINTEXT của broker 1 ra host

  kafk-2:
    image: apache/kafka:3.7.0
    container_name: kafk-2
    hostname: kafk-2
    networks:
      - traffic_net
    environment:
      - KAFKA_BROKER_ID=2
      - KAFKA_NODE_ID=2
      - KAFKA_PROCESS_ROLES=broker,controller
      - KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CONTROLLER_QUORUM_VOTERS=1@kafk-1:9093,2@kafk-2:9093,3@kafk-3:9093
      - KAFKA_LISTENERS=INTERNAL://:9092,CONTROLLER://:9093,PLAINTEXT://:9094
      - KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafk-2:9092,PLAINTEXT://localhost:39092
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0
      - KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=2
      - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - KAFKA_LOG_DIRS=/tmp/kraft-combined_logs
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
    ports:
      - "39092:9094" # Map cổng PLAINTEXT của broker 2 ra host

  kafk-3:
    image: apache/kafka:3.7.0
    container_name: kafk-3
    hostname: kafk-3
    networks:
      - traffic_net
    environment:
      - KAFKA_BROKER_ID=3
      - KAFKA_NODE_ID=3
      - KAFKA_PROCESS_ROLES=broker,controller
      - KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CONTROLLER_QUORUM_VOTERS=1@kafk-1:9093,2@kafk-2:9093,3@kafk-3:9093
      - KAFKA_LISTENERS=INTERNAL://:9092,CONTROLLER://:9093,PLAINTEXT://:9094
      - KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafk-3:9092,PLAINTEXT://localhost:49092
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=3
      - KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0
      - KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=2
      - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=3
      - KAFKA_LOG_DIRS=/tmp/kraft-combined_logs
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
    ports:
      - "49092:9094" # Map cổng PLAINTEXT của broker 3 ra host

  # --- Kafka Connect (Cập nhật để dùng Avro & Schema Registry) ---
  connect:
    image: confluentinc/cp-kafka-connect:7.3.0 # Nên dùng version tương thích Kafka
    container_name: connect
    depends_on:
      - kafk-1
      - kafk-2
      - kafk-3
      - schema-registry # Thêm phụ thuộc vào Schema Registry
    networks:
      - traffic_net
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafk-1:9092,kafk-2:9092,kafk-3:9092 # Dùng listener nội bộ
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: connect-group-1
      CONNECT_CONFIG_STORAGE_TOPIC: connect_configs
      CONNECT_OFFSET_STORAGE_TOPIC: connect_offsets
      CONNECT_STATUS_STORAGE_TOPIC: connect_status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      # --- Thay đổi Converter ---
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter # Internal có thể vẫn dùng JSON
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components # Bao gồm AvroConverter
    ports:
      - "8083:8083"

  # --- Các dịch vụ khác (Redis, MinIO, ClickHouse - Giữ nguyên) ---
  redis:
    image: redis:7.0-alpine
    container_name: redis
    ports: ["6379:6379"]
    networks: [traffic_net]
    healthcheck: { test: ["CMD", "redis-cli", "ping"], interval: 10s, timeout: 5s, retries: 5 }
  minio:
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    container_name: minio
    ports: ["${MINIO_API_PORT:-9000}:9000", "${MINIO_CONSOLE_PORT:-9001}:9001"]
    volumes: [minio_data:/data]
    # Dạng block style map (khuyến nghị):
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    command: server /data --console-address ":9001"
    networks: [traffic_net]
    healthcheck: { test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"], interval: 30s, timeout: 20s, retries: 3 }
  clickhouse-server:
    image: clickhouse/clickhouse-server:23.8
    container_name: clickhouse-server
    ports: ["8123:8123", "19000:9000"]
    volumes: [clickhouse_data:/var/lib/clickhouse/, ./infra/clickhouse/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql]
    ulimits: { nofile: { soft: 262144, hard: 262144 } }
    networks: [traffic_net]
    healthcheck: { test: ["CMD", "wget", "--spider", "-q", "localhost:8123/ping"], interval: 30s, timeout: 5s, retries: 3 }

  # --- Airflow Setup ---
  # Service chạy lệnh init DB cho Airflow (chỉ chạy 1 lần)
  airflow-init:
    build:
      context: ./airflow # Tạo thư mục airflow ở gốc dự án
      dockerfile: Dockerfile
    container_name: airflow_init
    depends_on:
      postgres: { condition: service_healthy }
    entrypoint: /bin/bash
    command: -c "airflow db init && airflow users create --username ${AIRFLOW_USER:-airflow} --password ${AIRFLOW_PASSWORD:-airflow} --firstname Anonymous --lastname User --role Admin --email admin@example.com"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor # Dùng LocalExecutor cho đơn giản
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER:-airflow}:${AIRFLOW_DB_PASSWORD:-airflow}@postgres:5432/${AIRFLOW_DB_NAME:-airflow}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW_UID=50000 # Chạy với user ID này để tránh lỗi permission với volume mount
    user: "${AIRFLOW_UID:-50000}:0" # Chạy với user ID và group 0 (root)
    volumes: # Mount để init có thể thấy DAGs nếu cần (thường không cần)
      - ./dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      # - ./airflow/plugins:/opt/airflow/plugins # Nếu có plugin
    networks:
      - traffic_net

  # Airflow Webserver
  airflow-webserver:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow_webserver
    restart: always
    depends_on:
      postgres: { condition: service_healthy }
      airflow-init: { condition: service_completed_successfully } # Chỉ start sau khi init xong
    ports:
      - "8080:8080" # Port cho Airflow UI
    environment: # Cấu hình giống airflow-init
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER:-airflow}:${AIRFLOW_DB_PASSWORD:-airflow}@postgres:5432/${AIRFLOW_DB_NAME:-airflow}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW_UID=50000
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY:-super_secret_key_change_me} # Đặt secret key
      # Các biến môi trường để DAG có thể sử dụng khi chạy spark-submit
      - KAFKA_BROKERS_INTERNAL=kafk-1:9092,kafk-2:9092,kafk-3:9092
      - KAFKA_TOPIC=raw_traffic_data
      - SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - REDIS_HOST=redis
      - CLICKHOUSE_HOST=clickhouse-server
      - CLICKHOUSE_NATIVE_PORT=9000 # Port ClickHouse cho Spark
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ROOT_USER:-minioadmin}
      - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD:-minioadmin}
      - MINIO_BUCKET=traffic-data
    user: "${AIRFLOW_UID:-50000}:0"
    volumes: # Mount DAGs, logs để webserver có thể đọc
      - ./dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      # - ./airflow/plugins:/opt/airflow/plugins
      # Mount thư mục chứa Spark application để Airflow có thể submit
      - ./spark_processor/app:/opt/spark/app
      - ./schemas:/opt/spark/schemas
    command: airflow webserver
    networks:
      - traffic_net
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Airflow Scheduler
  airflow-scheduler:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow_scheduler
    restart: always
    depends_on:
      postgres: { condition: service_healthy }
      airflow-init: { condition: service_completed_successfully }
    environment: # Cấu hình giống airflow-init/webserver
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER:-airflow}:${AIRFLOW_DB_PASSWORD:-airflow}@postgres:5432/${AIRFLOW_DB_NAME:-airflow}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW_UID=50000
      # Các biến môi trường để DAG có thể sử dụng khi chạy spark-submit
      - KAFKA_BROKERS_INTERNAL=kafk-1:9092,kafk-2:9092,kafk-3:9092
      - KAFKA_TOPIC=raw_traffic_data
      - SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - REDIS_HOST=redis
      - CLICKHOUSE_HOST=clickhouse-server
      - CLICKHOUSE_NATIVE_PORT=9000
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ROOT_USER:-minioadmin}
      - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD:-minioadmin}
      - MINIO_BUCKET=traffic-data
    user: "${AIRFLOW_UID:-50000}:0"
    volumes: # Mount DAGs, logs, plugins và code Spark
      - ./dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      # - ./airflow/plugins:/opt/airflow/plugins
      - ./spark_processor/app:/opt/spark/app
      - ./schemas:/opt/spark/schemas
    command: airflow scheduler
    networks:
      - traffic_net
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob", "--hostname", "$${HOSTNAME}"]
      interval: 60s
      timeout: 30s
      retries: 5

  # --- Dịch vụ API (Không đổi) ---
  api_service: # Giữ nguyên như trước
    build: { context: ./api_service, dockerfile: Dockerfile }
    container_name: api_service
    depends_on: { redis: { condition: service_healthy }, clickhouse-server: { condition: service_healthy } }
    ports: ["${API_PORT:-8000}:8000"]
    environment: { REDIS_HOST: redis, REDIS_PORT: 6379, CLICKHOUSE_HOST: clickhouse-server, CLICKHOUSE_PORT: 8123, CLICKHOUSE_NATIVE_PORT: 9000 }
    volumes: [./api_service/app:/app]
    networks: [traffic_net]